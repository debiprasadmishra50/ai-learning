{
  "1312.3300v1": {
    "title": "Numerical Reproducibility and Parallel Computations: Issues for Interval Algorithms",
    "authors": [
      "Nathalie Revol",
      "Philippe Th\u00e9veny"
    ],
    "summary": "What is called \"numerical reproducibility\" is the problem of getting the same result when the scientific computation is run several times, either on the same machine or on different machines, with different types and numbers of processing units, execution environments, computational loads etc. This problem is especially stringent for HPC numerical simulations. In what follows, the focus is on parallel implementations of interval arithmetic using floating-point arithmetic. For interval computations, numerical reproducibility is of course an issue for testing and debugging purposes. However, as long as the computed result encloses the exact and unknown result, the inclusion property, which is the main property of interval arithmetic, is satisfied and getting bit for bit identical results may not be crucial. Still, implementation issues may invalidate the inclusion property. Several ways to preserve the inclusion property are presented, on the example of the product of matrices with interval coefficients.",
    "pdf_url": "https://arxiv.org/pdf/1312.3300v1",
    "published": "2013-12-11"
  },
  "2207.05241v1": {
    "title": "Accelerating Large-Scale Graph-based Nearest Neighbor Search on a Computational Storage Platform",
    "authors": [
      "Ji-Hoon Kim",
      "Yeo-Reum Park",
      "Jaeyoung Do",
      "Soo-Young Ji",
      "Joo-Young Kim"
    ],
    "summary": "K-nearest neighbor search is one of the fundamental tasks in various applications and the hierarchical navigable small world (HNSW) has recently drawn attention in large-scale cloud services, as it easily scales up the database while offering fast search. On the other hand, a computational storage device (CSD) that combines programmable logic and storage modules on a single board becomes popular to address the data bandwidth bottleneck of modern computing systems. In this paper, we propose a computational storage platform that can accelerate a large-scale graph-based nearest neighbor search algorithm based on SmartSSD CSD. To this end, we modify the algorithm more amenable on the hardware and implement two types of accelerators using HLS- and RTL-based methodology with various optimization methods. In addition, we scale up the proposed platform to have 4 SmartSSDs and apply graph parallelism to boost the system performance further. As a result, the proposed computational storage platform achieves 75.59 query per second throughput for the SIFT1B dataset at 258.66W power dissipation, which is 12.83x and 17.91x faster and 10.43x and 24.33x more energy efficient than the conventional CPU-based and GPU-based server platform, respectively. With multi-terabyte storage and custom acceleration capability, we believe that the proposed computational storage platform is a promising solution for cost-sensitive cloud datacenters.",
    "pdf_url": "https://arxiv.org/pdf/2207.05241v1",
    "published": "2022-07-12"
  },
  "2601.11095v1": {
    "title": "Towards Quantum-Resistant Trusted Computing: Architectures for Post-Quantum Integrity Verification Techniques",
    "authors": [
      "Grazia D'Onghia",
      "Antonio Lioy"
    ],
    "summary": "Trust is the core building block of secure systems, and it is enforced through methods to ensure that a specific system is properly configured and works as expected. In this context, a Root of Trust (RoT) establishes a trusted environment, where both data and code are authenticated via a digital signature based on asymmetric cryptography, which is vulnerable to the threat posed by Quantum Computers (QCs). Firmware, being the first layer of trusted software, faces unique risks due to its longevity and difficult update. The transition of firmware protection to Post-Quantum Cryptography (PQC) is urgent, since it reduces the risk derived from exposing all computing and network devices to quantum-based attacks. This paper offers an analysis of the most common trust techniques and their roadmap towards a Post-Quantum (PQ) world, by investigating the current status of PQC and the challenges posed by such algorithms in existing Trusted Computing (TC) solutions from an integration perspective. Furthermore, this paper proposes an architecture for TC techniques enhanced with PEC, addressing the imperative for immediate adoption of quantum-resistant algorithms.",
    "pdf_url": "https://arxiv.org/pdf/2601.11095v1",
    "published": "2026-01-16"
  },
  "2012.10468v1": {
    "title": "A Comprehensive Utility Function for Resource Allocation in Mobile Edge Computing",
    "authors": [
      "Zaiwar Ali",
      "Sadia Khaf",
      "Ziaul Haq Abba",
      "Ghulam Abbas",
      "Lei Jiao",
      "Amna Irshad",
      "Kyung Sup Kwak",
      "Muhammad Bilal"
    ],
    "summary": "In mobile edge computing (MEC), one of the important challenges is how much resources of which mobile edge server (MES) should be allocated to which user equipment (UE). The existing resource allocation schemes only consider CPU as the requested resource and assume utility for MESs as either a random variable or dependent on the requested CPU only. This paper presents a novel comprehensive utility function for resource allocation in MEC. The utility function considers the heterogeneous nature of applications that a UE offloads to MES. The proposed utility function considers all important parameters, including CPU, RAM, hard disk space, required time, and distance, to calculate a more realistic utility value for MESs. Moreover, we improve upon some general algorithms, used for resource allocation in MEC and cloud computing, by considering our proposed utility function. We name the improved versions of these resource allocation schemes as comprehensive resource allocation schemes. The UE requests are modeled to represent the amount of resources requested by the UE as well as the time for which the UE has requested these resources. The utility function depends upon the UE requests and the distance between UEs and MES, and serves as a realistic means of comparison between different types of UE requests. Choosing (or selecting) an optimal MES with the optimal amount of resources to be allocated to each UE request is a challenging task. We show that MES resource allocation is sub-optimal if CPU is the only resource considered. By taking into account the other resources, i.e., RAM, disk space, request time, and distance in the utility function, we demonstrate improvement in the resource allocation algorithms in terms of service rate, utility, and MES energy consumption.",
    "pdf_url": "https://arxiv.org/pdf/2012.10468v1",
    "published": "2020-12-18"
  },
  "2009.00041v1": {
    "title": "Design and Simulation of a Hybrid Architecture for Edge Computing in 5G and Beyond",
    "authors": [
      "Hamed Rahimi",
      "Yvan Picaud",
      "Salvatore Costanzo",
      "Giyyarpuram Madhusudan",
      "Olivier Boissier",
      "kamal Deep Singh"
    ],
    "summary": "Edge Computing in 5G and Beyond is a promising solution for ultra-low latency applications (e.g. Autonomous Vehicle, Augmented Reality, and Remote Surgery), which have an extraordinarily low tolerance for the delay and require fast data processing for a very high volume of data. The requirements of delay-sensitive applications (e.g. Low latency, proximity, and Location/Context-awareness) cannot be satisfied by Cloud Computing due to the high latency between User Equipment and Cloud. Nevertheless, Edge Computing in 5G and beyond can promise an ultra-high-speed caused by placing computation capabilities closer to endpoint devices, whereas 5G encourages the speed rate that is 200 times faster than 4G LTE-Advanced. This paper deeply investigates Edge Computing in 5G and characterizes it based on the requirements of ultra-low latency applications. As a contribution, we propose a hybrid architecture that takes advantage of novel and sustainable technologies (e.g. D2D communication, Massive MIMO, SDN, and NFV) and has major features such as scalability, reliability and ultra-low latency support. The proposed architecture is evaluated based on an agent-based simulation that demonstrates it can satisfy requirements and has the ability to respond to high volume demands with low latency.",
    "pdf_url": "https://arxiv.org/pdf/2009.00041v1",
    "published": "2020-08-31"
  },
  "2009.08005v1": {
    "title": "Model-based approach for analyzing prevalence of nuclear cataracts in elderly residents",
    "authors": [
      "Sachiko Kodera",
      "Akimasa Hirata",
      "Fumiaki Miura",
      "Essam A. Rashed",
      "Natsuko Hatsusaka",
      "Naoki Yamamoto",
      "Eri Kubo",
      "Hiroshi Sasaki"
    ],
    "summary": "Recent epidemiological studies have hypothesized that the prevalence of cortical cataracts is closely related to ultraviolet radiation. However, the prevalence of nuclear cataracts is higher in elderly people in tropical areas than in temperate areas. The dominant factors inducing nuclear cataracts have been widely debated. In this study, the temperature increase in the lens due to exposure to ambient conditions was computationally quantified in subjects of 50-60 years of age in tropical and temperate areas, accounting for differences in thermoregulation. A thermoregulatory response model was extended to consider elderly people in tropical areas. The time course of lens temperature for different weather conditions in five cities in Asia was computed. The temperature was higher around the mid and posterior part of the lens, which coincides with the position of the nuclear cataract. The duration of higher temperatures in the lens varied, although the daily maximum temperatures were comparable. A strong correlation (adjusted R2 > 0.85) was observed between the prevalence of nuclear cataract and the computed cumulative thermal dose in the lens. We propose the use of a cumulative thermal dose to assess the prevalence of nuclear cataracts. Cumulative wet-bulb globe temperature, a new metric computed from weather data, would be useful for practical assessment in different cities.",
    "pdf_url": "https://arxiv.org/pdf/2009.08005v1",
    "published": "2020-09-17"
  },
  "2510.18902v2": {
    "title": "Evaluating LLMs for Career Guidance: Comparative Analysis of Computing Competency Recommendations Across Ten African Countries",
    "authors": [
      "Precious Eze",
      "Stephanie Lunn",
      "Bruk Berhane"
    ],
    "summary": "Employers increasingly expect graduates to utilize large language models (LLMs) in the workplace, yet the competencies needed for computing roles across Africa remain unclear given varying national contexts. This study examined how six LLMs, namely ChatGPT 4, DeepSeek, Gemini, Claude 3.5, Llama 3, and Mistral AI, describe entry-level computing career expectations across ten African countries. Using the Computing Curricula 2020 framework and drawing on Digital Colonialism Theory and Ubuntu Philosophy, content analysis of 60 LLM responses to standardized prompts reveals consistent coverage of technical competencies such as cloud computing and programming, but notable differences in non-technical competencies, particularly ethics and responsible AI use. Models vary considerably in recognizing country-specific factors, including local technology ecosystems, language requirements, and national policies averaging only 35.4% contextual awareness overall. Open-source models demonstrated stronger contextual awareness and better balance between technical and professional skills, with Llama (4.47/5) and DeepSeek (4.25/5) outperforming proprietary alternatives ChatGPT-4 (3.90/5) and Claude (3.46/5). However, Mistral's poor contextual performance (0.00/4) despite being open-source indicates that development philosophy alone does not guarantee contextual responsiveness. This first comprehensive comparison of LLM career guidance for African computing students uncovers entrenched infrastructure assumptions and Western-centric biases that create gaps between technical recommendations and local realities. The findings challenge assumptions about AI tool quality in resource-constrained settings and underscore the need for decolonial approaches to AI in education, emphasizing contextual relevance and hybrid human-AI guidance models.",
    "pdf_url": "https://arxiv.org/pdf/2510.18902v2",
    "published": "2025-10-20"
  },
  "2511.00106v1": {
    "title": "Wayfinding through the AI wilderness: Mapping rhetorics of ChatGPT prompt writing on X (formerly Twitter) to promote critical AI literacies",
    "authors": [
      "Anuj Gupta",
      "Ann Shivers-McNair"
    ],
    "summary": "In this paper, we demonstrate how studying the rhetorics of ChatGPT prompt writing on social media can promote critical AI literacies. Prompt writing is the process of writing instructions for generative AI tools like ChatGPT to elicit desired outputs and there has been an upsurge of conversations about it on social media. To study this rhetorical activity, we build on four overlapping traditions of digital writing research in computers and composition that inform how we frame literacies, how we study social media rhetorics, how we engage iteratively and reflexively with methodologies and technologies, and how we blend computational methods with qualitative methods. Drawing on these four traditions, our paper shows our iterative research process through which we gathered and analyzed a dataset of 32,000 posts (formerly known as tweets) from X (formerly Twitter) about prompt writing posted between November 2022 to May 2023. We present five themes about these emerging AI literacy practices: (1) areas of communication impacted by prompt writing, (2) micro-literacy resources shared for prompt writing, (3) market rhetoric shaping prompt writing, (4) rhetorical characteristics of prompts, and (5) definitions of prompt writing. In discussing these themes and our methodologies, we highlight takeaways for digital writing teachers and researchers who are teaching and analyzing critical AI literacies.",
    "pdf_url": "https://arxiv.org/pdf/2511.00106v1",
    "published": "2025-10-30"
  },
  "2305.06450v1": {
    "title": "What Students Can Learn About Artificial Intelligence -- Recommendations for K-12 Computing Education",
    "authors": [
      "Tilman Michaeli",
      "Stefan Seegerer",
      "Ralf Romeike"
    ],
    "summary": "Technological advances in the context of digital transformation are the basis for rapid developments in the field of artificial intelligence (AI). Although AI is not a new topic in computer science (CS), recent developments are having an immense impact on everyday life and society. In consequence, everyone needs competencies to be able to adequately and competently analyze, discuss and help shape the impact, opportunities, and limits of artificial intelligence on their personal lives and our society. As a result, an increasing number of CS curricula are being extended to include the topic of AI. However, in order to integrate AI into existing CS curricula, what students can and should learn in the context of AI needs to be clarified. This has proven to be particularly difficult, considering that so far CS education research on central concepts and principles of AI lacks sufficient elaboration. Therefore, in this paper, we present a curriculum of learning objectives that addresses digital literacy and the societal perspective in particular. The learning objectives can be used to comprehensively design curricula, but also allow for analyzing current curricula and teaching materials and provide insights into the central concepts and corresponding competencies of AI.",
    "pdf_url": "https://arxiv.org/pdf/2305.06450v1",
    "published": "2023-05-10"
  },
  "1906.05352v1": {
    "title": "Uncovering Dominant Social Class in Neighborhoods through Building Footprints: A Case Study of Residential Zones in Massachusetts using Computer Vision",
    "authors": [
      "Qianhui Liang",
      "Zhoutong Wang"
    ],
    "summary": "In urban theory, urban form is related to social and economic status. This paper explores to uncover zip-code level income through urban form by analyzing figure-ground map, a simple, prevailing and precise representation of urban form in the field of urban study. Deep learning in computer vision enables such representation maps to be studied at a large scale. We propose to train a DCNN model to identify and uncover the internal bridge between social class and urban form. Further, using hand-crafted informative visual features related with urban form properties (building size, building density, etc.), we apply a random forest classifier to interpret how morphological properties are related with social class.",
    "pdf_url": "https://arxiv.org/pdf/1906.05352v1",
    "published": "2019-06-12"
  }
}